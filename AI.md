# AI

## Ch.1 搜索问题

### 1.1 深度优先搜索

**优先扩展深度深的节点**

- **Depth-First-Search** **(DATA)**

  **DATA：当前状态。**
	**返回值：从当前状态到目标状态的路径（以规则表的形式表示）或FAIL。**
  
  ```lisp
  Depth-First-Search(DATA)
  IF TERM(DATA) RETURN NIL;
  IF DEADEND(DATA) RETURN FAIL;
  RULES:=APPRULES(DATA);
  LOOP: IF NULL(RULES) RETURN FAIL;
  R:=FIRST(RULES);
  RULES:=TAIL(RULES);
  RDATA:=GEN(R, DATA);
  PATH:= Depth-First-Search(RDATA);
  IF PATH=FAIL GO LOOP;
  RETURN CONS(R, PATH);   
  ```
  
- 存在问题及解决办法

  - 问题：
    - 深度问题
    - 死循环问题
  - 解决办法：
    - 对搜索深度加以限制
    - 记录从初始状态到当前状态的路径

- **Depth-First-Search1(DATALIST)**

  **DATALIST：从初始到当前的状态表（逆向）**
  **返回值：从当前状态到目标状态的路径（以规则表的形式表示）或FAIL。**
  
  ```lisp
  Depth-First-Search1(DATALIST)
  DATA:=FIRST(DATALIST)
  IF MENBER(DATA, TAIL(DATALIST))
  	RETURN FAIL;	
  IF TERM(DATA) RETURN NIL;
  IF DEADEND(DATA) RETURN FAIL;
  IF LENGTH(DATALIST)>BOUND
  	RETURN FAIL;
  RULES:=APPRULES(DATA);
  LOOP: IF NULL(RULES) RETURN FAIL;
  R:=FIRST(RULES);
  RULES:=TAIL(RULES);
  RDATA:=GEN(R, DATA);
  RDATALIST:=CONS(RDATA, DATALIST);
  PATH:= Depth-First-Search 1(RDATALIST)
  IF PATH=FAIL GO LOOP;
  RETURN CONS(R, PATH);
  ```
  
- 深度优先搜索的性质

  - 一般不能保证找到最优解
  - 当深度限制不合理时，可能找不到解，可以将算法改为可变深度限制
  - 最坏情况是，搜索空间等同于穷举
  - 是一个通用的与问题无关的方法
  - 节省内存，只存储从初始节点到当前节点到路径

### 1.2 宽度优先搜索

**优先扩展深度浅的节点**

- **BREADTH-FIRST-SEARCH**

  ```lisp
  BREADTH-FIRST-SEARCH
  G:=G0(G0=s), OPEN:=(s), CLOSED:=( );
  LOOP: IF OPEN=( ) THEN EXIT (FAIL);
  n:=FIRST(OPEN);
  IF GOAL(n) THEN EXIT (SUCCESS);
  REMOVE(n, OPEN), ADD(n, CLOSED);
  EXPAND(n) →{mi}, G:=ADD(mi, G);
  IF 目标在{mi}中 THEN EXIT(SUCCESS);
  ADD(OPEN, mj), 并标记mj到n的指针;
  GO LOOP;
  ```

- 宽度优先搜索的性质

  - 当问题有解时，一定能找到解
  - 当问题为单位耗散值，且问题有解时，一定能找到最优解
  - 方法与问题无关，具有通用性
  - 效率较低
  - 存储量比较大

EX 代价树的BFS

### 1.3 启发式图搜索

**优先扩展“最佳”节点**

- 利用知识来引导搜索，达到减少搜索范围，降低问题复杂度的目的。

- 启发信息的强度

  - 强：降低搜索工作量，但可能导致找不到最优解
  - 弱：一般导致工作量加大，极限情况下变为盲目搜索，但可能可以找到最优解

- 引入启发知识，在保证找到最佳解的情况下，尽可能减少搜索范围，提高搜索效率。

- 基本思想：

  定义一个评价函数$f$，对当前的搜索状态进行评估，找出一个最有希望的节点来扩展

- 启发式搜索算法$A$（$A$算法）

  - 评价函数的格式：
    $$
    f(n)=g(n)+h(n)\\
    f(n):评价函数\\
    h(n):启发函数
    $$

  - 符号的意义：

    - $g^*(n):$从s到n的最短路径的耗散值
    - $h^*(n):$从n到g的最短路径的耗散值
    - $f^*(n)=g^*(n)+h^*(n):$从s经过n到g的最短路径的耗散值
    - $g(n),h(n),f(n)$分别是$g^*(n),h^*(n),f^*(n)$的估计值
    - 用$f(n)$对待扩展节点进行评价
    - 实质是对当前搜索路径的评价

  ```lisp
  ;open表：未扩展的节点
  ;close表：已扩展或正在扩展的节点
  ;优先级队列结构：以估价函数f的递增次序排列OPEN表中的节点，估价函数低的排在前
  OPEN:=(s), f(s):=g(s)+h(s);
  LOOP: IF OPEN=() THEN EXIT(FAIL);
  n:=FIRST(OPEN);
  IF GOAL(n) THEN EXIT(SUCCESS);
  REMOVE(n, OPEN), ADD(n, CLOSED);
  EXPAND(n) →{mi}, 计算f(n, mi):=g(n, mi)+h(mi);
  ADD(mj, OPEN),
  	;标记mj到n的指针；
  	IF f(n, mk)<f(mk) THEN f(mk):=f(n, mk), 
  	           ;标记mk到n的指针；
  	IF f(n, ml)<f(ml,) THEN f(ml):=f(n, ml),
  	          ;标记ml到n的指针, 
  	          ADD(ml, OPEN);
  ;OPEN中的节点按f值从小到大排序；
  GO LOOP；
  ```

- 最佳图搜索算法$A^*$($A^*$算法)

  - 在$A$算法中，如果满足条件：$h(n)\le h^*(n)$，则$A$算法称为$A^*$算法

  - 定理（可采纳性定理）：

    若存在从初始节点s到目标节点t的路径，则$A^*$必能找到最佳解结束

  - 定理：

    设对同一个问题定义了两个$A^*$算法$A_1$和$A_2$，若$A_2$比$A_1$有较多的启发信息，即对所有非目标节点有$h_2(n) > h_1(n)$，则在具有一条从s到t的路径的隐含图上，搜索结束时，由$A2$所扩展的每一个节点，也必定由$A1$所扩展，即$A_1$扩展的节点数至少和$A2$一样多。

    简写：如果$h_2(n) > h_1(n)$（目标节点除外），则$A_1$扩展的节点数$\ge A_2$扩展的节点数

    **注意**：上述定理，评价指标是“扩展的节点数”，也就是说，同一个节点无论被扩展多少次，都只计算一次。

  - 单调函数：

    若h(x)对任两个节点的评价之差都不大于这两个节点的距离( h(i)-h(j)<=distance(i,j)，这里距离是有向的（？）)，且h(t)=0（？），则称h(x)是单调的

    定理：单调启发函数使得A*扩展到目标节点时必能找到最佳路径

    与动态规划算法的联系：h(x)=0时A*退化为动态规划算法

- 拓展：与或图搜索

## Ch.2 对抗搜索

### 2.1 博弈问题

- 双人
- 一人一步
- 双方信息完备
- 零和

### 2.2 极小极大过程

博弈树：以博弈双方交替行动构成的所有可能情形为节点，可以构成一棵博弈树；初始情况对应根节点，某一方的一步行动等同于从一个节点转移到它的一个对应子节点。

估价函数：对每个端节点计算评价函数/估价函数值，用来衡量该节点对双方的有利程度。例如，把双方得分差作为估价函数。

倒推值与节点分类：

​	应用如下**极大极小分析法**，对树中每个节点计算得到的一个评价称倒推值：

​	极大节点/或节点：所有子节点的倒推值中选最大，作为自己的倒推值，对应己方行动

​	极小节点/与节点：所有子节点的倒推值中选最小，作为自己的倒推值，对应对手行动

​	特别地，端节点的倒推值即取其估价函数值本身。

### 2.3 $\alpha-\beta$剪枝

对于一个极小节点来说，它维护一个倒推值的上界，用已遍历子节点中的最小倒推值来更新，称此值为β值。

对于一个极大节点来说，它维护一个倒推值的下界，用已遍历子节点中的最大倒推值来更新，称此值为α值。

剪枝过程：

1. 任何极大节点x的α值如果不能降低其某个祖先节点的β值，则对节点x以下的分枝可停止搜索，并使x的倒推值为α。这种剪枝成为β剪枝。

2. 任何极小节点x的β值如果不能升高其某个祖先节点的α值，则对节点x以下的分枝可停止搜索，并使x的倒推值为β。这种剪枝成为α剪枝。

### 2.4 蒙特卡洛博弈方法

布封投针实验

蒙特卡洛方法

​	维护：每个节点维护被模拟次数和收益的估计值

​	选择当前博弈树中一个可扩展的子节点 -> 扩展其子节点数目 -> 对新扩展的子节点模拟，得到其估计值 -> 根据极大极小方法，将模拟所得估计值作为增量向上依次更新祖先节点的估计值

```
function MCTSSEARCH(s_0)
    以状态s_0创建根节点v_0;
    while 尚未用完计算时长 do:
        v_l←TREEPOLICY(v_0);
        ∆←DEFAULTPOLICY(s(v_l));
        BACKUP(v_l,∆);
    end while
    return a(BESTCHILD(v_0));
```

多臂老虎机模型MAB

​	信心上限算法UCB1

信心上限树算法UCT

## Ch.3 高级搜索

### 3.1 局部搜索算法

- 基本思想：在搜索过程中，始终向着离目标最接近的方向搜索
- 目标可以是最大值，也可以是最小值

```lisp
局部搜索算法（Local Search）
随机的选择一个初始的可能解x0∈D，xb=x0，P=N(xb)；
如果不满足结束条件，则
Begin
	 选择P的一个子集P'，xn为P'中的最优解
	 如果f(xn) < f(xb)，则xb ＝ xn，P = N(xb)，转2；f(x)为指标函数。
	 否则P = P – P'，转2。
End
输出计算结果
结束
```

- **存在的问题**：局部最优问题

  **解决方法**：每次并不一定选择邻域内最优的点，而是依据一定的概率，从邻域内选择一个点，指标函数优的点，被选中的概率比较大，而指标函数差的点，被选中的概率比较小。通过引入随机的机制，有可能从局部最优解处跳出，但由于该算法会随机的选择一些不太好的点，因此有些情况下得到的结果可能会不太好，但总体上来说，效果会比一般的局部搜索算法好。

  **存在的问题**：步长问题

  **解决方法**：一种可行的方法是将固定步长的搜索方法变为动态步长，开始时选择比较大的步长，随着搜索的进行，逐步减小步长。这样既解决了固定步长所带来的问题，又在一定程度上解决了小步长搜索耗时的问题。

  **存在的问题**：起始点问题

  **解决方法**：随机的生成一些起始点，从每个起始点出发进行搜索，找到各自的最优解。再从这些最优解中选择一个最好的结果作为最终的结果。

### 3.2 模拟退火算法

​	模拟退火算法是局部搜索算法的一种扩展。模拟退火算法是根据复杂组合优化问题与固体的退火过程间的相似之处，在它们之间建立联系而提出来的。

#### 3.2.1 固体退火过程

​	在高温下，系统基本处于无序的状态，基本以等概率落入各个状态。在给定的温度下，系统落入低能量状态的概率大于系统落入高能量状态的概率，这样在同一温度下，如果系统交换的足够充分，则系统会趋向于落入较低能量的状态。随着温度的缓慢下降，系统落入低能量状态的概率逐步增加，而落入高能量状态的概率逐步减少，使得系统各状态能量的期望值随温度的下降单调下降，而只有那些能量小于期望值的状态，其概率才随温度下降增加，其他状态均随温度下降而下降。因此，随着能量期望值的逐步下降，能量低于期望值的状态逐步减少，当温度趋于0时，只剩下那些具有最小能量的状态，系统处于其他状态的概率趋近于0。因此最终系统将以概率1处于具有最小能量的一个状态。***（太长不看版：温度较高时系统处于各状态概率基本相等，随着温度降低系统会落入低能量状态的概率逐渐提高。温度趋于0时，只剩下最小能量的状态，其余状态概率趋近于0）***

固体退火过程，最终达到最小能量的一个状态（最优解），从理论上来说，必须满足以下三个条件：

1. 初始温度必须足够高；
2. 在每个温度下，状态的交换必须足够充分；
3. 温度T的下降必须足够缓慢。



#### 3.2.2 模拟退火算法

```lisp
1，随机选择一个解i，k=0，t0=Tmax（初始温度），计算指标函数f(i)。
2，如果满足结束条件，则转（15）。
3，Begin
4，	如果在该温度内达到了平衡条件，则转（13）。
5，	Begin
6，	    从i的邻域N(i)中随机选择一个解j。
7，	    计算指标函数f(j)。
8，	    如果f(j)<f(i)，则i=j，f(i)=f(j)，转（4）。

9，	    计算Pt(i=>j)

10，	    如果 Random(0, 1)<Pt(i=>j)，则i=j，f(i)=f(j)。
11，	    转（4）
12，	End
13，	tk+1=Drop(tk)，k=k+1。
14，End
15，输出结果。
16，结束。
```

​	该算法有内外两层循环。内循环模拟的是在给定温度下系统达到热平衡的过程。每次循环随机的产生一个新解，然后按照Metropolis准则，随机的接受该解。算法中的Random(0, 1)，是一个在[0, 1]间均匀分布的随机数发生器，与从解i到劣解j的转移概率相结合，模拟系统是否接受了劣解j。外循环模拟的是温度的下降过程，控制参数$t_k$起到与温度T相类似的作用，表示的是第k次循环时系统所处的温度。算法中的$Drop(t_k)$是一个温度下降函数，它按照一定的原则实施温度的缓慢下降。

​	上述模拟退火算法只是给出了一个算法的框架，其中重要的三个条件：初始温度的选取，内循环的结束条件和外循环的结束条件，算法中都没有提及，而这正是模拟退火算法的关键所在。

- 起始温度$t_0$的选取

  模拟退火算法要求初始温度足够高，这样才能够使得在初始温度下，以等概率处于任何一个状态。一个合适的初始温度，应保证平稳分布中每一个状态的概率基本相等，也就是接受概率$P_0$近似等于1。

- 温度下降方法

  1. 等比例下降

     该方法简单实用，是一种常用的温度下降方法。

  2. 等值下降

     该方法的好处是可以控制总的温度下降次数，但由于每次温度下降的是一个固定值，如果设置过小，在高温时温度下降太慢，如果设置的大，在低温下温度下降的又过快。

  3. 基于距离参数的下降方法
     $$
     t_{k+1}=\frac{t_{k}}{1+\frac{t_{k} \ln (1+\delta)}{3 \sigma_{t_{k}}}}
     $$
     1、2两种方法独立于具体的问题，而方法3是与具体的问题有关的温度下降方法。

- 每一温度下的停止准则

  1. 固定长度方法

     这是最简单的一种方法，在每一个温度下，都使用相同的$L_k$。$L_k$的选取与具体的问题相关，一般与邻域的大小直接关联，通常选择为问题规模n的一个多项式函数。

  2. 基于接受率的停止准则

     在高温时，即便比较小的迭代数，也可以基本达到平稳状态。而随着温度的下降，被拒绝的状态数随之增加，因此在低温下迭代数应增加，以免由于迭代数太少，而过早的陷入局部最优状态。因此一个直观的想法就是随着温度的下降适当的增加迭代次数。

     一种方法就是，规定一个接受次数R，在某一温度下，只有被接受的状态数达到R时，在该温度下的迭代才停止，转入下一个温度。由于随着温度的下降，状态被接受的概率随之下降，因此这样的一种准则是满足随着温度的下降适当的增加迭代次数的。但由于在温度比较低时，接受概率很低，为了防止出现过多的迭代次数，一般设置一个迭代次数的上限，当迭代次数达到上限时，即便不满足接受次数R，也停止这一温度的迭代过程。

     与上一种方法相类似的，可以规定一个状态接受率R，R等于该温度下接受的状态数除以生成的总状态数。如果接受率达到了R，则停止该温度下的迭代，转入下一个温度。为了防止迭代次数过少或者过多，一般定义一个迭代次数的下限和上限，只有当迭代次数达到了下限并且满足所要求的接受率R时，或者达到了迭代次数的上限时，才停止这一温度的迭代。

     还可以通过引入“代”的概念来定义停止准则。在迭代的过程中，若干相邻的状态称为“一代”，如果相邻两代的解的指标函数差值小于规定的值的话，则停止该温度下的迭代。

- 算法的终止原则

  1. 零度法

     从理论上讲，当温度趋近于0时，模拟退火算法才结束。因此，可以设定一个正常数$\varepsilon$，当$t_k<\varepsilon$时，算法结束。

  2. 循环总控制法

     给定一个指定的温度下降次数K，当温度的迭代次数达到K次时，则算法停止。这要求给定一个合适的K。如果K值选择不合适，对于小规模问题将导致增加算法无谓的运行时间，而对于大规模问题，则可能难于得到高质量的解。

  3. 无变化控制法

     随着温度的下降，虽然由于模拟退火算法会随机的接受一些不好的解，但从总体上来说，得到的解的质量应该逐步提高，在温度比较低时，更是如此。如果在相邻的n个温度中，得到的解的指标函数值无任何变化，则说明算法已经收敛。即便是收敛于局部最优解，由于在低温下跳出局部最优解的可能性很小，因此算法可以终止。

  4. 接受概率控制法

     给定一个小的概率值p，如果在当前温度下除了局部最优状态外，其他状态的接受概率小于p值，则算法结束。

  5. 领域平均概率控制法

  6. 相对误差估计法

     5和6比较复杂，建议看一下教材

### 3.3 遗传算法

#### 3.3.1 遗传算法的三个主要操作

- 选择

  “轮盘赌”法：设群体的规模为N，$F(x_i)(i=1, ..., N)$是其中N个染色体的适应值。则第i个染色体被选中的概率由下式给出：
  $$
  p\left(x_{i}\right)=\frac{F\left(x_{i}\right)}{\sum_{j=1}^{N} F\left(x_{j}\right)}
  $$

  ```lisp
  （1）r=random(0, 1)，s=0，i=0；
  （2）如果s≥r，则转（4）；
  （3）s=s+p(xi)，转（2）
  （4）xi即为被选中的染色体，输出i；
  （5）结束。
  ```

  “确定性”法：对于规模为N的群体，一个选择概率为$p(x_i)$的染色体$x_i$被选择次数的期望值$e(x_i)$：
  $$
  e(x_i)=p(x_i)N
  $$
  对于群体中的每一个$x_i$，首先选择$\lfloor e(x_i)\rfloor$次。这样共得到$\sum_{i=1}^{N}\lfloor e(x_i)\rfloor$个染色体。然后按照$e(x_i)-\lfloor e(x_i)\rfloor$从大到小对染色体排序，依次取出$N-\sum_{i=1}^{N}\lfloor e(x_i)\rfloor$个染色体，这样就得到了$N$个染色体。

- 交叉

  交叉发生在两个染色体之间，由两个被称之为双亲的父代染色体，经杂交以后，产生两个具有双亲的部分基因的新的染色体。当染色体采用二进制形式编码时，交叉过程是以这样一种形式进行的：
  $$
  \begin{array}{lllllll}{a_{1}} & {a_{2}} & {\dots} & {a_{i}} & {a_{i+1}} & {\dots} & {a_{n}} \\ 
  {b_{1}} & {b_{2}} & {\dots} & {b_{i}} & {b_{i+1}} & {\dots} & {b_{n}}\end{array}
  \to
  \begin{array}{lllllll}{a_{1}} & {a_{2}} & {\dots} & {a_{i}} & {b_{i+1}} & {\dots} & {b_{n}} \\ 
  {b_{1}} & {b_{2}} & {\dots} & {b_{i}} & {a_{i+1}} & {\dots} & {a_{n}}\end{array}
  $$
  在进化过程中，通常交叉是以一定的概率发生，而不是100％的发生。

- 变异

  变异发生在染色体的某一个基因上，当以二进制编码时，变异的基因由0变成1，或者由1变成0。如对于染色体x=11001，如果变异位发生在第三位，则变异后的染色体变成了y=11101。

  ```lisp
  （1）给定群体规模N，交叉概率pc和变异概率pm，t＝0；
  （2）随机生成N个染色体作为初始群体；
  （3）对于群体中的每一个染色体xi(i=1, 2, ..., N)分别计算其适应值F(xi)；
  （4）如果算法满足停止准则，则转（10）；
  （5）对群体中的每一个染色体xi依式（60）计算概率；
  （6）依据计算得到的概率值，从群体中随机的选取N个染色体，得到种群；
  （7）依据交叉概率pc从种群中选择染色体进行交叉，其子代进入新的群体，种群中未进行交叉的染色体，直接复制到新群体中；
  （8）依据变异概率pm从新群体中选择染色体进行变异，用变异后的染色体代替新群体中的原染色体；
  （9）用新群体代替旧群体，t=t+1，转（3）；
  （10）进化过程中适应值最大的染色体，经解码后作为最优解输出；
  （11）结束。
  ```

- 收敛性定理：如果在代的进化过程中，遗传算法每次保留到目前为止的最好解，并且算法以交叉和变异为其随机化操作，则对于一个全局最优化问题，当进化代数趋于无穷时，遗传算法找到最优解的概率为1。

#### 3.3.2 遗传算法的实现问题

- 编码问题

  将问题的解以适合于遗传算法求解的形式进行编码，称为遗传算法的表示。而交叉、变异等操作是与编码的形式有关的。因此在对问题进行编码时，要考虑到交叉和变异问题。最简单的编码是二进制形式，此外还有整数编码、实数编码、树编码等。

- 遗传算法的评价

  1. 当前最好法

     该方法在每一代进化过程中，记录得到的最好解，通过最好解的变化，了解算法的变化趋势。不同的算法之间，也可以通过该最好解的变化情况进行横向比较。

  2. 在线比较法

     该方法用当前代中染色体的平均指标函数值来刻划算法的变化趋势。计算方法如下：
     $$
     v_{on\_line}=\frac{1}{T} \sum_{t=1}^{T} f(t)
     $$
     其中$T$为当前代中染色体的个数，$f(t)$为第$t$个染色体的指标函数值。

     在以最大化为问题的优化目标时，在进化过程中，每代的值可能会出现一些波动，但总的趋势应该是上升的，并逐渐趋于稳定。

  3. 离线比较法

     该方法与在线比较法有些相似，但是用进化过程中每代最好解的指标函数值的平均值，来评价算法的进化过程。计算方法如下：
     $$
     v_{ off\_line}=\frac{1}{T} \sum_{t=1}^{T} f^*(t)
     $$
     其中$T$是到目前为止的进化代数，$f^*(t)$是第$t$代中，染色体的最好指标函数值。在以最大化为问题的优化目标时，随着算法的进化，该值具有上升的趋势。

     以上每一种方法，都可以监控算法的进化趋势，掌握遗传算法的进化情况，从而决定算法是否停止。

- 适应函数

- 二进制编码的交叉规则

  1. 双亲双子法
  2. 变化交叉法
  3. 多交叉位法
  4. 双亲单子法

- 整数编码的交叉规则

  1. 常规交叉法
  2. 基于次序的交叉法
  3. 基于位置的交叉法

- 变异规则

  1. 基于位置的变异
  2. 基于次序的变异
  3. 打乱变异

- 性能评价

## Ch.4 统计机器学习方法

### 4.1 朴素贝叶斯法

任务定义：设有X，Y两个随机变量，我们认为Y与X具有相关性，并已独立地测得充分多组X，Y取值的数据，训练一个分类器，使其在已知X某个取值的情况下可以选出发生可能性最大的Y。

原理：贝叶斯公式 极大似然估计

等价于取最大化P(X=x,Y=C_K)/P(Y=C_K)=P(Y=C_K|X=x)P(X=x)/P(Y=C_K)的参数C_K作为输出，在X确定的情况下，P(X=x)不妨约去

拉普拉斯平滑+λ

例题：用某表的数据学习一个naive bayes分类器，并确定类标记Y

SOLUTION 把所有的P(X|Y)，P(Y)等算出来就行

### 4.2 支持向量机（SVM）

分类：线性可分支持向量机 线性支持向量机 非线性支持向量机（核技巧）

任务：训练集由一组向量x（样本点）构成，已知每个向量x具有一个取值为1或-1的标签y，要求根据训练集训练一个分类器，使得对给定的向量x可以给出对其标签的判断。

基本思想：训练一个超平面 y=wx+b的参数w、b，其输出正负为预测结果。

对于一个超平面y=wx+b及一个训练集，定义间隔如下：

​	函数间隔：$y(wx+b)$ （超平面关于样本点(x,y)的）函数间隔，\mathrm{min}_{x,y}\{y(wx+b)\} （超平面关于训练集T的）函数间隔

​	几何间隔：$\gamma = min y(wx+b)/|w|$

目标函数：$maximize _{w,b}\{γ\}$，可以直观作图理解：我们希望让超平面两侧的点到面的距离尽可能大

由于最后取的是输出符号，所以scale其实不重要，不妨做归一化简化问题：取超平面关于训练集的函数间隔γ|w|=1

最终目标函数转化为$minimize  |w|^2/2, \forall y(wx+b) \ge 1$

EX 已知若干正负例，求最大间隔超平面

SOLUTION 列出所有约束，然后变成规划问题（向量模长最小化）

**关于对偶算法的推导（帮助记忆公式）**

线性可分SVM原始问题是一个带约束的最小值问题：$\arg\min_{W,b}(\frac{1}{2}\left|\left|W\right|\right|)^2,\,y_i(W^\mathrm{T}X_i+b),\,\forall i\in 1,2,\cdots,N$

使用Lagrange乘子可以转化为无约束最值问题，其中$\alpha=(\alpha_1\;\alpha_2\;\cdots\;\alpha_N)^\mathrm{T}$：
$$
\mathcal{L}(W,b,\alpha)=\arg\max_{\alpha}\arg\min_{W,b}(\frac{1}{2}\left|\left|W\right|\right|^2-\sum_{i=1}^N \alpha_i(y_i(W^\mathrm{T}X_i+b)-1)
$$
改写$\frac{1}{2}\left|\left|W\right|\right|^2=\frac{1}{2}W^\mathrm{T}W=\frac{1}{2}W^\mathrm{T}\boldsymbol{I}W$，利用二次型求导的相关结论，可得到
$$
\frac{\partial{\mathcal{L}}}{\partial{W}}=W^\mathrm{T}-\sum_{i=1}^N(\alpha_iy_iX_i^{\mathrm{T}})\\
\frac{\partial{\mathcal{L}}}{\partial{b}}=-\sum_{i=1}^N \alpha_iy_i
$$
令上述偏导为0（可以再求一下Hessian矩阵，是半正定矩阵，左上角为$\boldsymbol{I}$，其他部分均为0，从而关于$W,\,b$能取最小值），代入$\mathcal{L}(W,b,\alpha)$，消去$W,\,b$，得到
$$
\mathcal{L}=\arg\max_{\alpha}(-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jX_i^\mathrm{T}X_j+\sum_{i=1}^N\alpha_i)=\arg\min_{\alpha}(\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jX_i^\mathrm{T}X_j-\sum_{i=1}^N\alpha_i)\\
\mbox{其中}\sum_{i=1}^N\alpha_iy_i=0,\,\alpha_i\ge 0,\,\forall i=1,2,\cdots,N
$$
由此可以方便地解出Lagrange乘子向量$\alpha$。令第一个偏导为0时得到$W^\mathrm{T}$的计算式，两边取转置，得到
$$
W=\sum_{i=1}^N \alpha_iy_iX_i
$$
代入超平面方程，得到
$$
b=y_j-W^\mathrm{T}X_j
$$
此$X_j$应为支持向量，对应$\alpha_j>0$。



线性支持向量机 松弛变量和惩罚项

非线性支持向量机：输入空间映射到希尔伯特空间，曲面映射到超平面

正定核， Gram矩阵半正定

（待补充）KKT条件

### 4.3 决策树

问题：训练集包括若干样本，每个样本具有固定数量的若干特征以及一个需要决策的目标特征，根据训练集训练一棵能较好按照目标特征分类的决策树

ID3：信息熵和条件信息熵（类似全概率？）公式，评估按某个特征划分当前数据集后对按目标分类的信息增益。

​	存在问题：容易过拟合；倾向选择分支多的分类

​	解决思路：DEF 信息增益比

C4.5：信息增益比和连续值

利用验证集剪枝

## Ch.5 神经网络与深度学习

## Ch.6 谓词演算及应用

### 6.1 归结原理

- 子句集

- 化子句集的方法

  1. 消蕴涵符
     $$
     a\to b \Rightarrow \neg a \lor b
     $$

  2. 移动否定符
     $$
     \neg(a\or b)\Rightarrow \neg a \land \neg b\\
     \neg(a\land b)\Rightarrow \neg a \lor \neg b\\
     \neg(\exists x)P(x)\Rightarrow (\forall x)\neg P(x)\\
     \neg(\forall x)P(x)\Rightarrow (\exists x)\neg P(x)
     $$

  3. 变量标准化

     即对于不同的约束，对应于不同的变量

  4. 量词左移

  5. 消存在量词（skolem化）

     原则：对于一个受存在量词约束的变量，如果它全程不受全称量词约束，则该变量用一个常量代替，如果它受全称量词约束，则该变量用一个函数代替

  6. 化为合取范式

  7. 隐去全称量词

  8. 表示为子句集

  9. 变量标准化（变量换名）

- 归结原理

  定理：若$S$是合式公式$F$的子句集，则$F$永假的充要条件是$S$不可满足

  S不可满足：若$nil\in S$，则$S$不可满足

### 6.2 归结方法

### 6.3 谓词逻辑的归结原理

- 置换

  $S=\{t_1/v_1,t_2/v_2,\dots,t_n/v_n\}$

  对公式$E$实施置换$S$后得到的公式称为$E$的例，记作$E_S$。

- 合一

  如果存在一个$S$置换，使得$\{E_i\}$中

  $E_{1s}=E_{2s}=E_{3s}=\dots=E_{ns}$

  则称$\{E_i\}$是可合一的。$S$为$\{E_i\}$的合一者。

  结论：合一者不唯一。

- 最一般合一者（mgu）

  置换最少，限制最少，产生的例最具有一般性。

  mgu也不是唯一的。

- 对于子句$C_1\lor L_1$和$C_2\lor L_2$，如果$L_1$与$\neg L_2$可合一，且$S$是其合一者，则$(C_1\lor C_2)_S$是其归结式。

### 6.4 基于归结的问答系统

- 提取回答的过程
  - 先进行归结，证明结论的正确性
  - 用重言式代替结论求反得到的子句
  - 按照证明过程进行归结
  - 最后，在原来为空的地方，得到的就是提取的回答
  - 修改后的证明树称为**修改证明树**

### 6.5 基于规则的逆向演绎系统

- 问题：
  - 归结方法不自然
  - 可能会丢失蕴含关系中所包含的控制信息
- 形式上的要求
  - 目标为任意表达式
  - 事实表达式是文字的**合取**
  - 规则形式：$L\to W$，其中$W$为单文字
  - 如形为：$L\to W_1 \land W_2$，则变换为：$L\to W_1$和$L\to W_2$
  - 变量受全称量词约束
  - 对目标逆向使用规则
- **目标**用Skolem化的**对偶形式**，即
  - 消去全称量词，用Skolem函数代替
  - 保留存在量词
  - 对主析取元作变量换名
- 对规则的处理

### 6.6 一些深入的问题

- 修剪不一致的局部解图

