# AI

## Ch.1 搜索问题

### 1.1 深度优先搜索

**优先扩展深度深的节点**

- **Depth-First-Search** **(DATA)**

  **DATA：当前状态。**
	**返回值：从当前状态到目标状态的路径（以规则表的形式表示）或FAIL。**
  
  ```lisp
  Depth-First-Search(DATA)
  IF TERM(DATA) RETURN NIL;
  IF DEADEND(DATA) RETURN FAIL;
  RULES:=APPRULES(DATA);
  LOOP: IF NULL(RULES) RETURN FAIL;
  R:=FIRST(RULES);
  RULES:=TAIL(RULES);
  RDATA:=GEN(R, DATA);
  PATH:= Depth-First-Search(RDATA);
  IF PATH=FAIL GO LOOP;
  RETURN CONS(R, PATH);   
  ```
  
- 存在问题及解决办法

  - 问题：
    - 深度问题
    - 死循环问题
  - 解决办法：
    - 对搜索深度加以限制
    - 记录从初始状态到当前状态的路径

- **Depth-First-Search1(DATALIST)**

  **DATALIST：从初始到当前的状态表（逆向）**
  **返回值：从当前状态到目标状态的路径（以规则表的形式表示）或FAIL。**
  
  ```lisp
  Depth-First-Search1(DATALIST)
  DATA:=FIRST(DATALIST)
  IF MENBER(DATA, TAIL(DATALIST))
  	RETURN FAIL;	
  IF TERM(DATA) RETURN NIL;
  IF DEADEND(DATA) RETURN FAIL;
  IF LENGTH(DATALIST)>BOUND
  	RETURN FAIL;
  RULES:=APPRULES(DATA);
  LOOP: IF NULL(RULES) RETURN FAIL;
  R:=FIRST(RULES);
  RULES:=TAIL(RULES);
  RDATA:=GEN(R, DATA);
  RDATALIST:=CONS(RDATA, DATALIST);
  PATH:= Depth-First-Search 1(RDATALIST)
  IF PATH=FAIL GO LOOP;
  RETURN CONS(R, PATH);
  ```
  
- 深度优先搜索的性质

  - 一般不能保证找到最优解
  - 当深度限制不合理时，可能找不到解，可以将算法改为可变深度限制
  - 最坏情况是，搜索空间等同于穷举
  - 是一个通用的与问题无关的方法
  - 节省内存，只存储从初始节点到当前节点到路径

### 1.2 宽度优先搜索

**优先扩展深度浅的节点**

- **BREADTH-FIRST-SEARCH**

  ```lisp
  BREADTH-FIRST-SEARCH
  G:=G0(G0=s), OPEN:=(s), CLOSED:=( );
  LOOP: IF OPEN=( ) THEN EXIT (FAIL);
  n:=FIRST(OPEN);
  IF GOAL(n) THEN EXIT (SUCCESS);
  REMOVE(n, OPEN), ADD(n, CLOSED);
  EXPAND(n) →{mi}, G:=ADD(mi, G);
  IF 目标在{mi}中 THEN EXIT(SUCCESS);
  ADD(OPEN, mj), 并标记mj到n的指针;
  GO LOOP;
  ```

- 宽度优先搜索的性质

  - 当问题有解时，一定能找到解
  - 当问题为单位耗散值，且问题有解时，一定能找到最优解
  - 方法与问题无关，具有通用性
  - 效率较低
  - 存储量比较大

### 1.3 启发式图搜索

**优先扩展“最佳”节点**

- 利用知识来引导搜索，达到减少搜索范围，降低问题复杂度的目的。

- 启发信息的强度

  - 强：降低搜索工作量，但可能导致找不到最优解
  - 弱：一般导致工作量加大，极限情况下变为盲目搜索，但可能可以找到最优解

- 引入启发知识，在保证找到最佳解的情况下，尽可能减少搜索范围，提高搜索效率。

- 基本思想：

  定义一个评价函数$f$，对当前的搜索状态进行评估，找出一个最有希望的节点来扩展

- 启发式搜索算法$A$（$A$算法）

  - 评价函数的格式：
    $$
    f(n)=g(n)+h(n)\\
    f(n):评价函数\\
    g(n):启发函数
    $$

  - 符号的意义：

    - $g^*(n):$从s到n的最短路径的耗散值
    - $h^*(n):$从n到g的最短路径的耗散值
    - $f^*(n)=g^*(n)+h^*(n):$从s经过n到g的最短路径的耗散值
    - $g(n),h(n),f(n)$分别是$g^*(n),h^*(n),f^*(n)$的估计值
    - 用$f(n)$对待扩展节点进行评价
  
  ```lisp
  OPEN:=(s), f(s):=g(s)+h(s);
  LOOP: IF OPEN=() THEN EXIT(FAIL);
  n:=FIRST(OPEN);
  IF GOAL(n) THEN EXIT(SUCCESS);
  REMOVE(n, OPEN), ADD(n, CLOSED);
  EXPAND(n) →{mi}, 计算f(n, mi):=g(n, mi)+h(mi);
  ADD(mj, OPEN),
  	标记mj到n的指针；
  	IF f(n, mk)<f(mk) THEN f(mk):=f(n, mk), 
  	           标记mk到n的指针；
  	IF f(n, ml)<f(ml,) THEN f(ml):=f(n, ml),
  	          标记ml到n的指针, 
  	          ADD(ml, OPEN);
  OPEN中的节点按f值从小到大排序；
  GO LOOP；
  ```

- 最佳图搜索算法$A^*$($A^*$算法)

  - 在$A$算法中，如果满足条件：$h(n)\le h^*(n)$，则$A$算法称为$A^*$算法

  - 定理（可采纳性定理）：

    若存在从初始节点s到目标节点t的路径，则$A^*$必能找到最佳解结束

  - 定理：

    设对同一个问题定义了两个$A^*$算法$A_1$和$A_2$，若$A_2$比$A_1$有较多的启发信息，即对所有非目标节点有$h_2(n) > h_1(n)$，则在具有一条从s到t的路径的隐含图上，搜索结束时，由$A2$所扩展的每一个节点，也必定由$A1$所扩展，即$A_1$扩展的节点数至少和$A2$一样多。
  
    简写：如果$h_2(n) > h_1(n)$（目标节点除外），则$A_1$扩展的节点数$\ge A_2$扩展的节点数
    
    **注意**：上述定理，评价指标是“扩展的节点数”，也就是说，同一个节点无论被扩展多少次，都只计算一次。
  

## Ch.2 对抗搜索

### 2.1 博弈问题

- 双人
- 一人一步
- 双方信息完备
- 零和

### 2.2 极小极大过程

### 2.3 $\alpha-\beta$剪枝

### 2.4 蒙特卡洛博弈方法

## Ch.3 高级搜索

### 3.1 局部搜索算法

- 基本思想：在搜索过程中，始终向着离目标最接近的方向搜索
- 目标可以是最大值，也可以是最小值

```lisp
局部搜索算法（Local Search）
随机的选择一个初始的可能解x0∈D，xb=x0，P=N(xb)；
如果不满足结束条件，则
Begin
	 选择P的一个子集P'，xn为P'中的最优解
	 如果f(xn) < f(xb)，则xb ＝ xn，P = N(xb)，转2；f(x)为指标函数。
	 否则P = P – P'，转2。
End
输出计算结果
结束
```

- **存在的问题**：局部最优问题

  **解决方法**：每次并不一定选择邻域内最优的点，而是依据一定的概率，从邻域内选择一个点，指标函数优的点，被选中的概率比较大，而指标函数差的点，被选中的概率比较小。通过引入随机的机制，有可能从局部最优解处跳出，但由于该算法会随机的选择一些不太好的点，因此有些情况下得到的结果可能会不太好，但总体上来说，效果会比一般的局部搜索算法好。

  **存在的问题**：步长问题

  **解决方法**：一种可行的方法是将固定步长的搜索方法变为动态步长，开始时选择比较大的步长，随着搜索的进行，逐步减小步长。这样既解决了固定步长所带来的问题，又在一定程度上解决了小步长搜索耗时的问题。

  **存在的问题**：起始点问题

  **解决方法**：随机的生成一些起始点，从每个起始点出发进行搜索，找到各自的最优解。再从这些最优解中选择一个最好的结果作为最终的结果。

### 3.2 模拟退火算法

​	模拟退火算法是局部搜索算法的一种扩展。模拟退火算法是根据复杂组合优化问题与固体的退火过程间的相似之处，在它们之间建立联系而提出来的。

#### 3.2.1 固体退火过程

​	在高温下，系统基本处于无序的状态，基本以等概率落入各个状态。在给定的温度下，系统落入低能量状态的概率大于系统落入高能量状态的概率，这样在同一温度下，如果系统交换的足够充分，则系统会趋向于落入较低能量的状态。随着温度的缓慢下降，系统落入低能量状态的概率逐步增加，而落入高能量状态的概率逐步减少，使得系统各状态能量的期望值随温度的下降单调下降，而只有那些能量小于期望值的状态，其概率才随温度下降增加，其他状态均随温度下降而下降。因此，随着能量期望值的逐步下降，能量低于期望值的状态逐步减少，当温度趋于0时，只剩下那些具有最小能量的状态，系统处于其他状态的概率趋近于0。因此最终系统将以概率1处于具有最小能量的一个状态。

固体退火过程，最终达到最小能量的一个状态，从理论上来说，必须满足以下三个条件：

1. 初始温度必须足够高；

2. 在每个温度下，状态的交换必须足够充分；

3. 温度T的下降必须足够缓慢。

#### 3.2.2 模拟退火算法

```lisp
1，随机选择一个解i，k=0，t0=Tmax（初始温度），计算指标函数f(i)。
2，如果满足结束条件，则转（15）。
3，Begin
4，	如果在该温度内达到了平衡条件，则转（13）。
5，	Begin
6，	    从i的邻域N(i)中随机选择一个解j。
7，	    计算指标函数f(j)。
8，	    如果f(j)<f(i)，则i=j，f(i)=f(j)，转（4）。

9，	    计算Pt(i=>j)

10，	    如果 Random(0, 1)<Pt(i=>j)，则i=j，f(i)=f(j)。
11，	    转（4）
12，	End
13，	tk+1=Drop(tk)，k=k+1。
14，End
15，输出结果。
16，结束。
```

​	该算法有内外两层循环。内循环模拟的是在给定温度下系统达到热平衡的过程。每次循环随机的产生一个新解，然后按照Metropolis准则，随机的接受该解。算法中的Random(0, 1)，是一个在[0, 1]间均匀分布的随机数发生器，与从解i到劣解j的转移概率相结合，模拟系统是否接受了劣解j。外循环模拟的是温度的下降过程，控制参数$t_k$起到与温度T相类似的作用，表示的是第k次循环时系统所处的温度。算法中的$Drop(t_k)$是一个温度下降函数，它按照一定的原则实施温度的缓慢下降。

​	上述模拟退火算法只是给出了一个算法的框架，其中重要的三个条件：初始温度的选取，内循环的结束条件和外循环的结束条件，算法中都没有提及，而这正是模拟退火算法的关键所在。

- 起始温度$t_0$的选取

  模拟退火算法要求初始温度足够高，这样才能够使得在初始温度下，以等概率处于任何一个状态。一个合适的初始温度，应保证平稳分布中每一个状态的概率基本相等，也就是接受概率$P_0$近似等于1。

- 温度

## Ch.4 统计机器学习方法

## Ch.5 神经网络与深度学习

## Ch.6 谓词演算及应用